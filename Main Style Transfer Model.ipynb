{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3feb202c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils as utils\n",
    "from video_utils import create_video_from_intermediate_results\n",
    "\n",
    "import torch\n",
    "from torch.optim import Adam, LBFGS\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import os\n",
    "import argparse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "609c7685",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_loss(neural_net, optimizing_img, target_representations, content_feature_maps_index, style_feature_maps_indices, config):\n",
    "    target_content_representation = target_representations[0]\n",
    "    target_style_representation = target_representations[1]\n",
    "\n",
    "    current_set_of_feature_maps = neural_net(optimizing_img)\n",
    "\n",
    "    current_content_representation = current_set_of_feature_maps[content_feature_maps_index].squeeze(axis=0)\n",
    "    content_loss = torch.nn.MSELoss(reduction='mean')(target_content_representation, current_content_representation)\n",
    "\n",
    "    style_loss = 0.0\n",
    "    current_style_representation = [utils.gram_matrix(x) for cnt, x in enumerate(current_set_of_feature_maps) if cnt in style_feature_maps_indices]\n",
    "    for gram_gt, gram_hat in zip(target_style_representation, current_style_representation):\n",
    "        style_loss += torch.nn.MSELoss(reduction='sum')(gram_gt[0], gram_hat[0])\n",
    "    style_loss /= len(target_style_representation)\n",
    "\n",
    "    tv_loss = utils.total_variation(optimizing_img)\n",
    "\n",
    "    total_loss = config['content_weight'] * content_loss + config['style_weight'] * style_loss + config['tv_weight'] * tv_loss\n",
    "\n",
    "    return total_loss, content_loss, style_loss, tv_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "761c0c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tuning_step(neural_net, optimizer, target_representations, content_feature_maps_index, style_feature_maps_indices, config):\n",
    "    # Builds function that performs a step in the tuning loop\n",
    "    def tuning_step(optimizing_img):\n",
    "        total_loss, content_loss, style_loss, tv_loss = build_loss(neural_net, optimizing_img, target_representations, content_feature_maps_index, style_feature_maps_indices, config)\n",
    "        # Computes gradients\n",
    "        total_loss.backward()\n",
    "        # Updates parameters and zeroes gradients\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        return total_loss, content_loss, style_loss, tv_loss\n",
    "\n",
    "    # Returns the function that will be called inside the tuning loop\n",
    "    return tuning_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a88b551",
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_style_transfer(config):\n",
    "    content_img_path = os.path.join(config['content_images_dir'], config['content_img_name'])\n",
    "    style_img_path = os.path.join(config['style_images_dir'], config['style_img_name'])\n",
    "\n",
    "    out_dir_name = 'combined_' + os.path.split(content_img_path)[1].split('.')[0] + '_' + os.path.split(style_img_path)[1].split('.')[0]\n",
    "    dump_path = os.path.join(config['output_img_dir'], out_dir_name)\n",
    "    os.makedirs(dump_path, exist_ok=True)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    content_img = utils.prepare_img(content_img_path, config['height'], device)\n",
    "    style_img = utils.prepare_img(style_img_path, config['height'], device)\n",
    "\n",
    "    if config['init_method'] == 'random':\n",
    "        # white_noise_img = np.random.uniform(-90., 90., content_img.shape).astype(np.float32)\n",
    "        gaussian_noise_img = np.random.normal(loc=0, scale=90., size=content_img.shape).astype(np.float32)\n",
    "        init_img = torch.from_numpy(gaussian_noise_img).float().to(device)\n",
    "    elif config['init_method'] == 'content':\n",
    "        init_img = content_img\n",
    "    else:\n",
    "        # init image has same dimension as content image - this is a hard constraint\n",
    "        # feature maps need to be of same size for content image and init image\n",
    "        style_img_resized = utils.prepare_img(style_img_path, np.asarray(content_img.shape[2:]), device)\n",
    "        init_img = style_img_resized\n",
    "\n",
    "    # we are tuning optimizing_img's pixels! (that's why requires_grad=True)\n",
    "    optimizing_img = Variable(init_img, requires_grad=True)\n",
    "\n",
    "    neural_net, content_feature_maps_index_name, style_feature_maps_indices_names = utils.prepare_model(config['model'], device)\n",
    "    print(f'Using {config[\"model\"]} in the optimization procedure.')\n",
    "\n",
    "    content_img_set_of_feature_maps = neural_net(content_img)\n",
    "    style_img_set_of_feature_maps = neural_net(style_img)\n",
    "\n",
    "    target_content_representation = content_img_set_of_feature_maps[content_feature_maps_index_name[0]].squeeze(axis=0)\n",
    "    target_style_representation = [utils.gram_matrix(x) for cnt, x in enumerate(style_img_set_of_feature_maps) if cnt in style_feature_maps_indices_names[0]]\n",
    "    target_representations = [target_content_representation, target_style_representation]\n",
    "\n",
    "    # magic numbers in general are a big no no - some things in this code are left like this by design to avoid clutter\n",
    "    num_of_iterations = {\n",
    "        \"lbfgs\": 1000,\n",
    "        \"adam\": 3000,\n",
    "    }\n",
    "\n",
    "    if config['optimizer'] == 'adam':\n",
    "        optimizer = Adam((optimizing_img,), lr=1e1)\n",
    "        tuning_step = make_tuning_step(neural_net, optimizer, target_representations, content_feature_maps_index_name[0], style_feature_maps_indices_names[0], config)\n",
    "        for cnt in range(num_of_iterations[config['optimizer']]):\n",
    "            total_loss, content_loss, style_loss, tv_loss = tuning_step(optimizing_img)\n",
    "            with torch.no_grad():\n",
    "                print(f'Adam | iteration: {cnt:03}, total loss={total_loss.item():12.4f}, content_loss={config[\"content_weight\"] * content_loss.item():12.4f}, style loss={config[\"style_weight\"] * style_loss.item():12.4f}, tv loss={config[\"tv_weight\"] * tv_loss.item():12.4f}')\n",
    "                utils.save_and_maybe_display(optimizing_img, dump_path, config, cnt, num_of_iterations[config['optimizer']], should_display=False)\n",
    "    elif config['optimizer'] == 'lbfgs':\n",
    "        # line_search_fn does not seem to have significant impact on result\n",
    "        optimizer = LBFGS((optimizing_img,), max_iter=num_of_iterations['lbfgs'], line_search_fn='strong_wolfe')\n",
    "        cnt = 0\n",
    "\n",
    "        def closure():\n",
    "            nonlocal cnt\n",
    "            if torch.is_grad_enabled():\n",
    "                optimizer.zero_grad()\n",
    "            total_loss, content_loss, style_loss, tv_loss = build_loss(neural_net, optimizing_img, target_representations, content_feature_maps_index_name[0], style_feature_maps_indices_names[0], config)\n",
    "            if total_loss.requires_grad:\n",
    "                total_loss.backward()\n",
    "            with torch.no_grad():\n",
    "                print(f'L-BFGS | iteration: {cnt:03}, total loss={total_loss.item():12.4f}, content_loss={config[\"content_weight\"] * content_loss.item():12.4f}, style loss={config[\"style_weight\"] * style_loss.item():12.4f}, tv loss={config[\"tv_weight\"] * tv_loss.item():12.4f}')\n",
    "                utils.save_and_maybe_display(optimizing_img, dump_path, config, cnt, num_of_iterations[config['optimizer']], should_display=False)\n",
    "\n",
    "            cnt += 1\n",
    "            return total_loss\n",
    "\n",
    "        optimizer.step(closure)\n",
    "\n",
    "    return dump_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "52687fa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--content_img_name CONTENT_IMG_NAME]\n",
      "                             [--style_img_name STYLE_IMG_NAME]\n",
      "                             [--height HEIGHT]\n",
      "                             [--content_weight CONTENT_WEIGHT]\n",
      "                             [--style_weight STYLE_WEIGHT]\n",
      "                             [--tv_weight TV_WEIGHT]\n",
      "                             [--optimizer {lbfgs,adam}]\n",
      "                             [--model {vgg16,vgg19}]\n",
      "                             [--init_method {random,content,style}]\n",
      "                             [--saving_freq SAVING_FREQ]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: --f=/Users/piyushpathak/Library/Jupyter/runtime/kernel-v32edbc5dd8c82558c64afaa35f140e9467cff2c5e.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py:3585: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    #\n",
    "    # fixed args - don't change these unless you have a good reason\n",
    "    #\n",
    "    default_resource_dir = './data'\n",
    "\n",
    "    content_images_dir = os.path.join(default_resource_dir, 'content-images')\n",
    "    style_images_dir = os.path.join(default_resource_dir, 'style-images')\n",
    "    output_img_dir = os.path.join(default_resource_dir, 'output-images')\n",
    "    img_format = (4, '.jpg')  # saves images in the format: %04d.jpg\n",
    "\n",
    "    #\n",
    "    # modifiable args - feel free to play with these (only small subset is exposed by design to avoid cluttering)\n",
    "    # sorted so that the ones on the top are more likely to be changed than the ones on the bottom\n",
    "    #\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--content_img_name\", type=str, help=\"content image name\", default='figures.jpg')\n",
    "    parser.add_argument(\"--style_img_name\", type=str, help=\"style image name\", default='vg_starry_night.jpg')\n",
    "    parser.add_argument(\"--height\", type=int, help=\"height of content and style images\", default=400)\n",
    "\n",
    "    parser.add_argument(\"--content_weight\", type=float, help=\"weight factor for content loss\", default=1e5)\n",
    "    parser.add_argument(\"--style_weight\", type=float, help=\"weight factor for style loss\", default=3e4)\n",
    "    parser.add_argument(\"--tv_weight\", type=float, help=\"weight factor for total variation loss\", default=1e0)\n",
    "\n",
    "    parser.add_argument(\"--optimizer\", type=str, choices=['lbfgs', 'adam'], default='lbfgs')\n",
    "    parser.add_argument(\"--model\", type=str, choices=['vgg16', 'vgg19'], default='vgg19')\n",
    "    parser.add_argument(\"--init_method\", type=str, choices=['random', 'content', 'style'], default='content')\n",
    "    parser.add_argument(\"--saving_freq\", type=int, help=\"saving frequency for intermediate images (-1 means only final)\", default=-1)\n",
    "    args = parser.parse_args()\n",
    "    optimization_config = dict()\n",
    "    for arg in vars(args):\n",
    "        optimization_config[arg] = getattr(args, arg)\n",
    "    optimization_config['content_images_dir'] = content_images_dir\n",
    "    optimization_config['style_images_dir'] = style_images_dir\n",
    "    optimization_config['output_img_dir'] = output_img_dir\n",
    "    optimization_config['img_format'] = img_format\n",
    "\n",
    "    # original NST (Neural Style Transfer) algorithm (Gatys et al.)\n",
    "    results_path = neural_style_transfer(optimization_config)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "837ffbe4",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (3479055917.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[16], line 2\u001b[0;36m\u001b[0m\n\u001b[0;31m    for arg in vars(args):\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96e18e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
