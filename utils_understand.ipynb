{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3920fd76",
   "metadata": {},
   "source": [
    "1️⃣ Loads and preprocesses images into tensors ready for VGG16/VGG19.\n",
    "2️⃣ Computes Gram matrix — captures the \"style\" of an image (texture, patterns).\n",
    "3️⃣ Computes Total Variation Loss — makes the generated image smoother.\n",
    "4️⃣ Provides functions to save and display intermediate or final images.\n",
    "5️⃣ Loads the correct VGG model and tells which layers to use for content/style features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9babd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26a328fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.definitions.vgg_nets import Vgg16, Vgg19, Vgg16Experimental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff05fa1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGENET_MEAN_255 = [123.675, 116.28, 103.53]\n",
    "IMAGENET_STD_NEUTRAL = [1, 1, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f8880d",
   "metadata": {},
   "source": [
    "Loads an image from disk using OpenCV.\n",
    "\n",
    "Converts BGR (OpenCV default) → RGB format.\n",
    "\n",
    "Optionally resizes the image (to given height or shape).\n",
    "\n",
    "Converts image data to float32 and scales it to range [0, 1].\n",
    "\n",
    "Returns the processed image as a numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c821d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(img_path, target_shape=None):\n",
    "    if not os.path.exists(img_path):\n",
    "        raise Exception(f'Path does not exist: {img_path}')\n",
    "    img = cv.imread(img_path)[:, :, ::-1]  # [:, :, ::-1] converts BGR (opencv format...) into RGB\n",
    "\n",
    "    if target_shape is not None:  # resize section\n",
    "        if isinstance(target_shape, int) and target_shape != -1:  # scalar -> implicitly setting the height\n",
    "            current_height, current_width = img.shape[:2]\n",
    "            new_height = target_shape\n",
    "            new_width = int(current_width * (new_height / current_height))\n",
    "            img = cv.resize(img, (new_width, new_height), interpolation=cv.INTER_CUBIC)\n",
    "        else:  # set both dimensions to target shape\n",
    "            img = cv.resize(img, (target_shape[1], target_shape[0]), interpolation=cv.INTER_CUBIC)\n",
    "\n",
    "    # this need to go after resizing - otherwise cv.resize will push values outside of [0,1] range\n",
    "    img = img.astype(np.float32)  # convert from uint8 to float32\n",
    "    img /= 255.0  # get to [0, 1] range\n",
    "    return img\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ffe09c",
   "metadata": {},
   "source": [
    "Uses load_image to load and resize the image.\n",
    "\n",
    "Converts image into a PyTorch tensor (transforms.ToTensor).\n",
    "\n",
    "Scales tensor back to [0, 255] and normalizes it using ImageNet mean.\n",
    "\n",
    "Moves tensor to the specified device (CPU or GPU).\n",
    "\n",
    "Returns a 4D tensor with batch dimension added (unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28106e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_img(img_path, target_shape, device):\n",
    "    img = load_image(img_path, target_shape=target_shape)\n",
    "\n",
    "    # normalize using ImageNet's mean\n",
    "    # [0, 255] range worked much better for me than [0, 1] range (even though PyTorch models were trained on latter)\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Lambda(lambda x: x.mul(255)),\n",
    "        transforms.Normalize(mean=IMAGENET_MEAN_255, std=IMAGENET_STD_NEUTRAL)\n",
    "    ])\n",
    "\n",
    "    img = transform(img).to(device).unsqueeze(0)\n",
    "\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c3b5e5",
   "metadata": {},
   "source": [
    "save_image(img, img_path)\n",
    "Saves a numpy image array to disk as an image file.\n",
    "\n",
    "If the image is grayscale (2D), it stacks channels to make it RGB.\n",
    "\n",
    "Converts RGB → BGR format for OpenCV saving.\n",
    "\n",
    "Calls cv.imwrite to save the image to disk.\n",
    "\n",
    "Very simple helper — used if you want to save any image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba98461c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_image(img, img_path):\n",
    "    if len(img.shape) == 2:\n",
    "        img = np.stack((img,) * 3, axis=-1)\n",
    "    cv.imwrite(img_path, img[:, :, ::-1])  # [:, :, ::-1] converts rgb into bgr (opencv contraint...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08463ab9",
   "metadata": {},
   "source": [
    "generate_out_img_name(config)\n",
    "Generates an output filename for saving images.\n",
    "\n",
    "Combines parts of content + style image names with hyperparameters.\n",
    "\n",
    "Adds optimizer name, model name, image height, weights, etc.\n",
    "\n",
    "If using reconstruction script, uses a simpler filename.\n",
    "\n",
    "Returns the full filename string — helps organize results clearly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4cbc9708",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_out_img_name(config):\n",
    "    prefix = os.path.basename(config['content_img_name']).split('.')[0] + '_' + os.path.basename(config['style_img_name']).split('.')[0]\n",
    "    # called from the reconstruction script\n",
    "    if 'reconstruct_script' in config:\n",
    "        suffix = f'_o_{config[\"optimizer\"]}_h_{str(config[\"height\"])}_m_{config[\"model\"]}{config[\"img_format\"][1]}'\n",
    "    else:\n",
    "        suffix = f'_o_{config[\"optimizer\"]}_i_{config[\"init_method\"]}_h_{str(config[\"height\"])}_m_{config[\"model\"]}_cw_{config[\"content_weight\"]}_sw_{config[\"style_weight\"]}_tv_{config[\"tv_weight\"]}{config[\"img_format\"][1]}'\n",
    "    return prefix + suffix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a40e96c",
   "metadata": {},
   "source": [
    "save_and_maybe_display(optimizing_img, dump_path, config, img_id, num_of_iterations, should_display=False)\n",
    "Saves intermediate or final optimized image to disk.\n",
    "\n",
    "Converts tensor to numpy array, adds back ImageNet mean, clips values to [0, 255].\n",
    "\n",
    "Decides when to save (based on saving_freq or final iteration).\n",
    "\n",
    "If should_display is True, shows image using matplotlib.\n",
    "\n",
    "Very useful in loop — shows training progress (like epochs).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8699065",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_and_maybe_display(optimizing_img, dump_path, config, img_id, num_of_iterations, should_display=False):\n",
    "    saving_freq = config['saving_freq']\n",
    "    out_img = optimizing_img.squeeze(axis=0).to('cpu').detach().numpy()\n",
    "    out_img = np.moveaxis(out_img, 0, 2)  # swap channel from 1st to 3rd position: ch, _, _ -> _, _, chr\n",
    "\n",
    "    # for saving_freq == -1 save only the final result (otherwise save with frequency saving_freq and save the last pic)\n",
    "    if img_id == num_of_iterations-1 or (saving_freq > 0 and img_id % saving_freq == 0):\n",
    "        img_format = config['img_format']\n",
    "        out_img_name = str(img_id).zfill(img_format[0]) + img_format[1] if saving_freq != -1 else generate_out_img_name(config)\n",
    "        dump_img = np.copy(out_img)\n",
    "        dump_img += np.array(IMAGENET_MEAN_255).reshape((1, 1, 3))\n",
    "        dump_img = np.clip(dump_img, 0, 255).astype('uint8')\n",
    "        cv.imwrite(os.path.join(dump_path, out_img_name), dump_img[:, :, ::-1])\n",
    "\n",
    "    if should_display:\n",
    "        plt.imshow(dump_img[:,:,::-1]) # convert bgr into rgb for matplotlib\n",
    "        #plt.imshow(np.uint8(get_uint8_range(out_img)))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0bf62d",
   "metadata": {},
   "source": [
    "prepare_model(model, device)\n",
    "Initializes and returns a VGG16 or VGG19 network (pretrained).\n",
    "\n",
    "Moves the model to device (CPU or GPU).\n",
    "\n",
    "Does NOT tune model weights — they are frozen.\n",
    "\n",
    "Returns the model and the layer indices for content + style loss.\n",
    "\n",
    "Essential step — sets up the model for style transfer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9709c4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_model(model, device):\n",
    "    # we are not tuning model weights -> we are only tuning optimizing_img's pixels! (that's why requires_grad=False)\n",
    "    experimental = False\n",
    "    if model == 'vgg16':\n",
    "        if experimental:\n",
    "            # much more flexible for experimenting with different style representations\n",
    "            model = Vgg16Experimental(requires_grad=False, show_progress=True)\n",
    "        else:\n",
    "            model = Vgg16(requires_grad=False, show_progress=True)\n",
    "    elif model == 'vgg19':\n",
    "        model = Vgg19(requires_grad=False, show_progress=True)\n",
    "    else:\n",
    "        raise ValueError(f'{model} not supported.')\n",
    "\n",
    "    content_feature_maps_index = model.content_feature_maps_index\n",
    "    style_feature_maps_indices = model.style_feature_maps_indices\n",
    "    layer_names = model.layer_names\n",
    "\n",
    "    content_fms_index_name = (content_feature_maps_index, layer_names[content_feature_maps_index])\n",
    "    style_fms_indices_names = (style_feature_maps_indices, layer_names)\n",
    "    return model.to(device).eval(), content_fms_index_name, style_fms_indices_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87996730",
   "metadata": {},
   "source": [
    "gram_matrix(x, should_normalize=True)\n",
    "Computes Gram matrix — captures \"style\" (texture, patterns).\n",
    "\n",
    "Takes a feature map tensor → flattens spatial dimensions.\n",
    "\n",
    "Computes dot-product of features with transpose.\n",
    "\n",
    "Normalizes the result (if enabled).\n",
    "\n",
    "Used for calculating style loss in NST (Neural Style Transfer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "752eba5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gram_matrix(x, should_normalize=True):\n",
    "    (b, ch, h, w) = x.size()\n",
    "    features = x.view(b, ch, w * h)\n",
    "    features_t = features.transpose(1, 2)\n",
    "    gram = features.bmm(features_t)\n",
    "    if should_normalize:\n",
    "        gram /= ch * h * w\n",
    "    return gram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed17d974",
   "metadata": {},
   "source": [
    "total_variation(y)\n",
    "Computes Total Variation Loss — a smoothness regularizer.\n",
    "\n",
    "Encourages neighboring pixels to have similar colors → less noise.\n",
    "\n",
    "Adds differences along horizontal and vertical directions.\n",
    "\n",
    "Returns a scalar — added to total loss during optimization.\n",
    "\n",
    "Helps avoid too many sharp edges / artifacts in final image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "61fab003",
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_variation(y):\n",
    "    return torch.sum(torch.abs(y[:, :, :, :-1] - y[:, :, :, 1:])) + \\\n",
    "           torch.sum(torch.abs(y[:, :, :-1, :] - y[:, :, 1:, :]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
