{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3920fd76",
   "metadata": {},
   "source": [
    "1️⃣ Loads and preprocesses images into tensors ready for VGG16/VGG19.\n",
    "\n",
    "2️⃣ Computes Gram matrix — captures the \"style\" of an image (texture, patterns).\n",
    "\n",
    "3️⃣ Computes Total Variation Loss — makes the generated image smoother.\n",
    "\n",
    "4️⃣ Provides functions to save and display intermediate or final images.\n",
    "\n",
    "5️⃣ Loads the correct VGG model and tells which layers to use for content/style features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9babd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26a328fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.definitions.vgg_nets import Vgg16, Vgg19, Vgg16Experimental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff05fa1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGENET_MEAN_255 = [123.675, 116.28, 103.53]\n",
    "IMAGENET_STD_NEUTRAL = [1, 1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c821d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(img_path, target_shape=None):\n",
    "    if not os.path.exists(img_path):\n",
    "        raise Exception(f'Path does not exist: {img_path}')\n",
    "    img = cv.imread(img_path)[:, :, ::-1]  # [:, :, ::-1] converts BGR (opencv format...) into RGB\n",
    "\n",
    "    if target_shape is not None:  # resize section\n",
    "        if isinstance(target_shape, int) and target_shape != -1:  # scalar -> implicitly setting the height\n",
    "            current_height, current_width = img.shape[:2]\n",
    "            new_height = target_shape\n",
    "            new_width = int(current_width * (new_height / current_height))\n",
    "            img = cv.resize(img, (new_width, new_height), interpolation=cv.INTER_CUBIC)\n",
    "        else:  # set both dimensions to target shape\n",
    "            img = cv.resize(img, (target_shape[1], target_shape[0]), interpolation=cv.INTER_CUBIC)\n",
    "\n",
    "    # this need to go after resizing - otherwise cv.resize will push values outside of [0,1] range\n",
    "    img = img.astype(np.float32)  # convert from uint8 to float32\n",
    "    img /= 255.0  # get to [0, 1] range\n",
    "    return img\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ffe09c",
   "metadata": {},
   "source": [
    "Loads an image from disk using OpenCV.\n",
    "\n",
    "Converts BGR (OpenCV default) → RGB format.\n",
    "\n",
    "Optionally resizes the image (to given height or shape).\n",
    "\n",
    "Converts image data to float32 and scales it to range [0, 1].\n",
    "\n",
    "Returns the processed image as a numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28106e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_img(img_path, target_shape, device):\n",
    "    img = load_image(img_path, target_shape=target_shape)\n",
    "\n",
    "    # normalize using ImageNet's mean\n",
    "    # [0, 255] range worked much better for me than [0, 1] range (even though PyTorch models were trained on latter)\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Lambda(lambda x: x.mul(255)),\n",
    "        transforms.Normalize(mean=IMAGENET_MEAN_255, std=IMAGENET_STD_NEUTRAL)\n",
    "    ])\n",
    "\n",
    "    img = transform(img).to(device).unsqueeze(0)\n",
    "\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c3b5e5",
   "metadata": {},
   "source": [
    "Uses load_image to load and resize the image.\n",
    "\n",
    "Converts image into a PyTorch tensor (transforms.ToTensor).\n",
    "\n",
    "Scales tensor back to [0, 255] and normalizes it using ImageNet mean.\n",
    "\n",
    "Moves tensor to the specified device (CPU or GPU).\n",
    "\n",
    "Returns a 4D tensor with batch dimension added (unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba98461c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_image(img, img_path):\n",
    "    if len(img.shape) == 2:\n",
    "        img = np.stack((img,) * 3, axis=-1)\n",
    "    cv.imwrite(img_path, img[:, :, ::-1])  # [:, :, ::-1] converts rgb into bgr (opencv contraint...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08463ab9",
   "metadata": {},
   "source": [
    "save_image(img, img_path)\n",
    "\n",
    "Saves a numpy image array to disk as an image file.\n",
    "\n",
    "If the image is grayscale (2D), it stacks channels to make it RGB.\n",
    "\n",
    "Converts RGB → BGR format for OpenCV saving.\n",
    "\n",
    "Calls cv.imwrite to save the image to disk.\n",
    "\n",
    "Very simple helper — used if you want to save any image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4cbc9708",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_out_img_name(config):\n",
    "    prefix = os.path.basename(config['content_img_name']).split('.')[0] + '_' + os.path.basename(config['style_img_name']).split('.')[0]\n",
    "    # called from the reconstruction script\n",
    "    if 'reconstruct_script' in config:\n",
    "        suffix = f'_o_{config[\"optimizer\"]}_h_{str(config[\"height\"])}_m_{config[\"model\"]}{config[\"img_format\"][1]}'\n",
    "    else:\n",
    "        suffix = f'_o_{config[\"optimizer\"]}_i_{config[\"init_method\"]}_h_{str(config[\"height\"])}_m_{config[\"model\"]}_cw_{config[\"content_weight\"]}_sw_{config[\"style_weight\"]}_tv_{config[\"tv_weight\"]}{config[\"img_format\"][1]}'\n",
    "    return prefix + suffix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a40e96c",
   "metadata": {},
   "source": [
    "generate_out_img_name(config)\n",
    "\n",
    "Generates an output filename for saving images\n",
    "\n",
    "Combines parts of content + style image names with hyperparameters.\n",
    "\n",
    "Adds optimizer name, model name, image height, weights, etc.\n",
    "\n",
    "If using reconstruction script, uses a simpler filename.\n",
    "\n",
    "Returns the full filename string — helps organize results clearly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8699065",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_and_maybe_display(optimizing_img, dump_path, config, img_id, num_of_iterations, should_display=False):\n",
    "    saving_freq = config['saving_freq']\n",
    "    out_img = optimizing_img.squeeze(axis=0).to('cpu').detach().numpy()\n",
    "    out_img = np.moveaxis(out_img, 0, 2)  # swap channel from 1st to 3rd position: ch, _, _ -> _, _, chr\n",
    "\n",
    "    # for saving_freq == -1 save only the final result (otherwise save with frequency saving_freq and save the last pic)\n",
    "    if img_id == num_of_iterations-1 or (saving_freq > 0 and img_id % saving_freq == 0):\n",
    "        img_format = config['img_format']\n",
    "        out_img_name = str(img_id).zfill(img_format[0]) + img_format[1] if saving_freq != -1 else generate_out_img_name(config)\n",
    "        dump_img = np.copy(out_img)\n",
    "        dump_img += np.array(IMAGENET_MEAN_255).reshape((1, 1, 3))\n",
    "        dump_img = np.clip(dump_img, 0, 255).astype('uint8')\n",
    "        cv.imwrite(os.path.join(dump_path, out_img_name), dump_img[:, :, ::-1])\n",
    "\n",
    "    if should_display:\n",
    "        plt.imshow(dump_img[:,:,::-1]) # convert bgr into rgb for matplotlib\n",
    "        #plt.imshow(np.uint8(get_uint8_range(out_img)))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0bf62d",
   "metadata": {},
   "source": [
    "save_and_maybe_display(optimizing_img, dump_path, config, img_id, num_of_iterations, should_display=False)\n",
    "\n",
    "Saves intermediate or final optimized image to disk.\n",
    "\n",
    "Converts tensor to numpy array, adds back ImageNet mean, clips values to [0, 255].\n",
    "\n",
    "Decides when to save (based on saving_freq or final iteration).\n",
    "\n",
    "If should_display is True, shows image using matplotlib.\n",
    "\n",
    "Very useful in loop — shows training progress (like epochs).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9709c4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_model(model, device):\n",
    "    # we are not tuning model weights -> we are only tuning optimizing_img's pixels! (that's why requires_grad=False)\n",
    "    experimental = False\n",
    "    if model == 'vgg16':\n",
    "        if experimental:\n",
    "            # much more flexible for experimenting with different style representations\n",
    "            model = Vgg16Experimental(requires_grad=False, show_progress=True)\n",
    "        else:\n",
    "            model = Vgg16(requires_grad=False, show_progress=True)\n",
    "    elif model == 'vgg19':\n",
    "        model = Vgg19(requires_grad=False, show_progress=True)\n",
    "    else:\n",
    "        raise ValueError(f'{model} not supported.')\n",
    "\n",
    "    content_feature_maps_index = model.content_feature_maps_index\n",
    "    style_feature_maps_indices = model.style_feature_maps_indices\n",
    "    layer_names = model.layer_names\n",
    "\n",
    "    content_fms_index_name = (content_feature_maps_index, layer_names[content_feature_maps_index])\n",
    "    style_fms_indices_names = (style_feature_maps_indices, layer_names)\n",
    "    return model.to(device).eval(), content_fms_index_name, style_fms_indices_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87996730",
   "metadata": {},
   "source": [
    "\n",
    "\\prepare_model(model, device)\n",
    "Initializes and returns a VGG16 or VGG19 network (pretrained).\n",
    "\n",
    "Moves the model to device (CPU or GPU).\n",
    "\n",
    "Does NOT tune model weights — they are frozen.\n",
    "\n",
    "Returns the model and the layer indices for content + style loss.\n",
    "\n",
    "Essential step — sets up the model for style transfer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "752eba5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gram_matrix(x, should_normalize=True):\n",
    "    (b, ch, h, w) = x.size()\n",
    "    features = x.view(b, ch, w * h)\n",
    "    features_t = features.transpose(1, 2)\n",
    "    gram = features.bmm(features_t)\n",
    "    if should_normalize:\n",
    "        gram /= ch * h * w\n",
    "    return gram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed17d974",
   "metadata": {},
   "source": [
    "\n",
    "gram_matrix(x, should_normalize=True)\n",
    "Computes Gram matrix — captures \"style\" (texture, patterns).\n",
    "\n",
    "Takes a feature map tensor → flattens spatial dimensions.\n",
    "\n",
    "Computes dot-product of features with transpose.\n",
    "\n",
    "Normalizes the result (if enabled).\n",
    "\n",
    "Used for calculating style loss in NST (Neural Style Transfer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "61fab003",
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_variation(y):\n",
    "    return torch.sum(torch.abs(y[:, :, :, :-1] - y[:, :, :, 1:])) + \\\n",
    "           torch.sum(torch.abs(y[:, :, :-1, :] - y[:, :, 1:, :]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40813f7c",
   "metadata": {},
   "source": [
    "total_variation(y)\n",
    "\n",
    "Computes Total Variation Loss — a smoothness regularizer.\n",
    "\n",
    "Encourages neighboring pixels to have similar colors → less noise.\n",
    "\n",
    "Adds differences along horizontal and vertical directions.\n",
    "\n",
    "Returns a scalar — added to total loss during optimization.\n",
    "\n",
    "Helps avoid too many sharp edges / artifacts in final image."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
